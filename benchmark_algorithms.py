### /benchmark_algorithms.py

import json
import time
import statistics
from genetic_algorithm import genetic_algorithm
from simulated_annealing import simulated_annealing

# --- Data Loading ---

# Loads all datasets from a JSON file generated by data_generator.py
def load_all_datasets(filepath):
    with open(filepath, "r") as f:
        return json.load(f)

# --- Single Algorithm Evaluation ---

# Benchmarks a single algorithm over multiple repetitions
# Records fitness, runtime, deviation, and overload from objective

def benchmark_algorithm(algo_fn, name, dataset, reps=10):
    results = []
    best_result = None
    best_fitness = float('inf')

    for _ in range(reps):
        start = time.time()
        result = algo_fn(dataset["projects"], dataset["project_managers"], dataset["weeks"])
        duration = time.time() - start
        result["runtime"] = duration
        results.append(result)

        if result["fitness"] < best_fitness:
            best_result = result
            best_fitness = result["fitness"]

    z_scores = [r["fitness"] for r in results]
    runtimes = [r["runtime"] for r in results]

    return {
        "algorithm": name,
        "dataset_id": dataset["dataset_id"],
        "mean_fitness": statistics.mean(z_scores),
        "std_fitness": statistics.stdev(z_scores) if len(z_scores) > 1 else 0.0,
        "mean_runtime": statistics.mean(runtimes),
        "std_runtime": statistics.stdev(runtimes) if len(runtimes) > 1 else 0.0,
        "mean_deviation": statistics.mean([r["deviation"] for r in results]),
        "mean_overload": statistics.mean([r["overload"] for r in results]),
        "best_solution": best_result["solution"]
    }

# --- Benchmark Runner ---

# Runs benchmark for all datasets using both GA and SA
# Stores results in an output file for further analysis

def run_benchmark(filepath, reps=10, output_file="benchmark_results.json"):
    datasets = load_all_datasets(filepath)
    all_results = []

    for dataset in datasets:
        print(f"Running benchmarks for dataset {dataset['dataset_id']}...")

        ga_metrics = benchmark_algorithm(genetic_algorithm, "GeneticAlgorithm", dataset, reps)
        sa_metrics = benchmark_algorithm(simulated_annealing, "SimulatedAnnealing", dataset, reps)

        all_results.extend([ga_metrics, sa_metrics])

    with open(output_file, "w") as out:
        json.dump(all_results, out, indent=2)

    print(f"\nSaved all benchmark results to {output_file}")


# --- Entry Point ---

if __name__ == "__main__":
    run_benchmark("datasets.json", reps=10)
